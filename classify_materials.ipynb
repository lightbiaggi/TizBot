{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can be improved but works ok for the moment\n",
    "\n",
    "import re\n",
    "\n",
    "def classify_and_extract(filename):\n",
    "    # Define file paths\n",
    "    journal_file = \"materials/papers/list_of_journals\"\n",
    "    webpage_file = \"materials/webpages/list_of_webpages\"\n",
    "    paper_references_file = \"materials/papers/article_references\"\n",
    "    website_references_file = \"materials/webpages/website_references\"\n",
    "    book_references_file = \"materials/books/books_references\"\n",
    "    other_resources_file = \"materials/other_resources/other_resources_references\"\n",
    "    \n",
    "    # Sets to track unique journal names, URLs, and references\n",
    "    unique_journals = set()\n",
    "    unique_webpages = set()\n",
    "    paper_references = []\n",
    "    website_references = []\n",
    "    book_references = []\n",
    "    other_resources = []\n",
    "\n",
    "    # Regex patterns\n",
    "    book_pattern = re.compile(r'.*(?:\\b(?:Press|University|Publisher|Books|Publications)\\b).*', re.IGNORECASE)\n",
    "    journal_pattern = re.compile(r'\\. ([A-Z][a-zA-Z\\s,&]+?)[\\.,]\\s*\\d{4}')\n",
    "    webpage_pattern = re.compile(r'https?://[^\\s,]+')\n",
    "    nih_pattern = re.compile(r'\\b(?:NIH|National Institutes of Health|NIMH|National Institute of Mental Health)\\b', re.IGNORECASE)\n",
    "\n",
    "    # Read the input file and process each line\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            cleaned_line = line.strip()\n",
    "\n",
    "            # Check if it's a webpage (contains a URL)\n",
    "            if \"http\" in cleaned_line:\n",
    "                url_match = webpage_pattern.search(cleaned_line)\n",
    "                if url_match:\n",
    "                    url = url_match.group(0)\n",
    "                    unique_webpages.add(url)\n",
    "                    website_references.append(cleaned_line)\n",
    "            \n",
    "            elif book_pattern.search(cleaned_line):\n",
    "                # Likely a book if it mentions terms related to publishing houses or universities\n",
    "                book_references.append(cleaned_line)\n",
    "            \n",
    "            elif journal_pattern.search(cleaned_line):\n",
    "                # Likely a paper if it matches the APA journal pattern\n",
    "                journal_match = journal_pattern.search(cleaned_line)\n",
    "                if journal_match:\n",
    "                    journal_name = journal_match.group(1).strip()\n",
    "                    unique_journals.add(journal_name)\n",
    "                    paper_references.append(cleaned_line)\n",
    "            \n",
    "            elif nih_pattern.search(cleaned_line):\n",
    "                # If it mentions NIH or similar, classify as a paper for now\n",
    "                paper_references.append(cleaned_line)\n",
    "            \n",
    "            else:\n",
    "                # If it doesn't match any specific pattern, consider it other resources\n",
    "                other_resources.append(cleaned_line)\n",
    "\n",
    "    # Write the unique journal names and URLs to their respective files\n",
    "    with open(journal_file, \"w\") as jf:\n",
    "        for journal in sorted(unique_journals):\n",
    "            jf.write(journal + \"\\n\")\n",
    "\n",
    "    with open(webpage_file, \"w\") as wf:\n",
    "        for webpage in sorted(unique_webpages):\n",
    "            wf.write(webpage + \"\\n\")\n",
    "\n",
    "    # Write all paper references to the article_references file\n",
    "    with open(paper_references_file, \"w\") as prf:\n",
    "        for reference in paper_references:\n",
    "            prf.write(reference + \"\\n\")\n",
    "\n",
    "    # Write all website and other material references to the website_references file\n",
    "    with open(website_references_file, \"w\") as wrf:\n",
    "        for reference in website_references:\n",
    "            wrf.write(reference + \"\\n\")\n",
    "\n",
    "    # Write all book references to the books_references file\n",
    "    with open(book_references_file, \"w\") as brf:\n",
    "        for reference in book_references:\n",
    "            brf.write(reference + \"\\n\")\n",
    "\n",
    "    # Write all other resources references to the other_resources file\n",
    "    with open(other_resources_file, \"w\") as orf:\n",
    "        for reference in other_resources:\n",
    "            orf.write(reference + \"\\n\")\n",
    "\n",
    "# Example usage\n",
    "filename = \"materials/assets_list\"\n",
    "classify_and_extract(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
